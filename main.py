#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SectorFlow Lite - Main Execution Script
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° í†µí•©

Usage:
    python main.py --mode [full|data|train|predict|backtest|report|cluster]
"""

import argparse
import sys
import os
import json
import hashlib
import random
import numpy as np
from datetime import datetime
import yaml
import warnings
warnings.filterwarnings('ignore')

# ì‹œë“œ ì„¤ì • í•¨ìˆ˜
def set_seeds(seed: int = 42):
    """ëª¨ë“  ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    
    # TensorFlow ì‹œë“œ ì„¤ì •
    try:
        import tensorflow as tf
        tf.random.set_seed(seed)
        tf.config.experimental.enable_op_determinism()
    except ImportError:
        pass
    
    # PyTorch ì‹œë“œ ì„¤ì •
    try:
        import torch
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    except ImportError:
        pass
    
    print(f"ğŸŒ± ì‹œë“œ ì„¤ì • ì™„ë£Œ: {seed}")

def generate_run_id() -> str:
    """ì‹¤í–‰ ID ìƒì„±"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    random_hash = hashlib.md5(str(random.random()).encode()).hexdigest()[:8]
    return f"{timestamp}_{random_hash}"

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'src'))

def load_config(config_path: str = "config.yaml") -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {config_path}")
        return config
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def setup_run_directory(run_id: str, config: dict) -> str:
    """ì‹¤í–‰ ë””ë ‰í† ë¦¬ ì„¤ì •"""
    runs_dir = f"runs/{run_id}"
    os.makedirs(runs_dir, exist_ok=True)
    
    # ì„¤ì • ìŠ¤ëƒ…ìƒ· ì €ì¥
    config_snapshot_path = os.path.join(runs_dir, "config_snapshot.yaml")
    with open(config_snapshot_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    # í™˜ê²½ ì •ë³´ ì €ì¥
    env_info = get_environment_info()
    env_info_path = os.path.join(runs_dir, "env_info.json")
    with open(env_info_path, 'w', encoding='utf-8') as f:
        json.dump(env_info, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ ì‹¤í–‰ ë””ë ‰í† ë¦¬ ì„¤ì •: {runs_dir}")
    return runs_dir

def get_environment_info() -> dict:
    """í™˜ê²½ ì •ë³´ ìˆ˜ì§‘"""
    import platform
    import subprocess
    
    env_info = {
        'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
        'platform': platform.platform(),
        'timestamp': datetime.now().isoformat()
    }
    
    # íŒ¨í‚¤ì§€ ë²„ì „ ì •ë³´
    packages = ['pandas', 'numpy', 'scikit-learn', 'matplotlib', 'seaborn']
    for pkg in packages:
        try:
            module = __import__(pkg)
            env_info[f'{pkg}_version'] = getattr(module, '__version__', 'N/A')
        except ImportError:
            env_info[f'{pkg}_version'] = 'N/A'
    
    # Git ì •ë³´
    try:
        git_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()[:8]
        env_info['git_hash'] = git_hash
    except:
        env_info['git_hash'] = 'N/A'
    
    return env_info

def run_data_pipeline(config: dict) -> dict:
    """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("\nğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
    
    from src.dataio import prepare_ml_data
    
    # ë°ì´í„° ì„¤ì •
    data_config = {
        'start_date': config.get('data', {}).get('start_date', '2024-01-01'),
        'end_date': config.get('data', {}).get('end_date', '2024-12-31'),
        'lookback': config.get('model', {}).get('sequence_length', 30),
        'feature_cols': config.get('features', {}).get('feature_cols', 
            ['close', 'volume', 'trading_value', 'returns', 'ma_5', 'ma_20', 'volatility']),
        'scale_method': 'standard'
    }
    
    symbols = config.get('data', {}).get('symbols', ['005930', '000660', '035420'])
    
    # ë°ì´í„° ì¤€ë¹„
    processed_data = prepare_ml_data(symbols, data_config)
    
    print(f"âœ… ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {len(processed_data)}ê°œ ì¢…ëª©")
    return processed_data

def run_baseline_training(processed_data: dict) -> dict:
    """ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨"""
    print("\nğŸ¤– ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    
    from src.baseline import train_all_baselines
    
    # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨
    baseline_results = train_all_baselines(processed_data)
    
    print(f"âœ… ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(baseline_results)}ê°œ ì¢…ëª©")
    return baseline_results

def run_deep_learning_training(processed_data: dict, config: dict) -> dict:
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨"""
    print("\nğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    
    try:
        from src.train import train_all_models
        
        # í›ˆë ¨ ì„¤ì •
        training_config = {
            'epochs': config.get('model', {}).get('epochs', 50),
            'batch_size': config.get('model', {}).get('batch_size', 32),
            'patience': 15,
            'learning_rate': config.get('model', {}).get('learning_rate', 0.001),
            'class_imbalance_method': 'class_weight'
        }
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
        dl_results = train_all_models(
            processed_data, 
            model_types=['gru', 'lstm'], 
            config=training_config
        )
        
        print(f"âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(dl_results)}ê°œ ì¢…ëª©")
        return dl_results
        
    except ImportError as e:
        print(f"âš ï¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ê±´ë„ˆëœ€: {e}")
        return {}

def run_backtesting(processed_data: dict, model_results: dict, config: dict) -> dict:
    """ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰"""
    print("\nğŸ’° ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰ ì¤‘...")
    
    from src.backtest import run_backtest, run_model_backtest
    from src.rules import generate_trading_signals
    from src.features import process_features
    
    backtest_results = {}
    
    for symbol, data in processed_data.items():
        print(f"   ğŸ“Š {symbol} ë°±í…ŒìŠ¤íŒ… ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        df = data['original_df'].copy()
        
        # 1. ë£° ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸
        try:
            # í”¼ì²˜ ê³„ì‚°
            df_with_features = process_features(df)
            
            # ì‹ í˜¸ ìƒì„±
            df_with_signals = generate_trading_signals(df_with_features)
            
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            rule_results = run_backtest(df_with_signals)
            backtest_results[f'{symbol}_rule_based'] = rule_results
            
        except Exception as e:
            print(f"   âŒ {symbol} ë£° ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 2. ëª¨ë¸ ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ (ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
        if symbol in model_results and 'models' in model_results[symbol]:
            try:
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
                X_test = data['X_test']
                best_model = model_results[symbol]['best_model']['model']
                
                # ì˜ˆì¸¡ í™•ë¥  ìƒì„±
                X_test_2d = X_test.reshape(X_test.shape[0], -1)
                predictions = best_model.predict_proba(X_test_2d)[:, 1]
                
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                model_results_backtest = run_model_backtest(df, predictions, threshold=0.5)
                backtest_results[f'{symbol}_model_based'] = model_results_backtest
                
            except Exception as e:
                print(f"   âŒ {symbol} ëª¨ë¸ ê¸°ë°˜ ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"âœ… ë°±í…ŒìŠ¤íŒ… ì™„ë£Œ: {len(backtest_results)}ê°œ ì „ëµ")
    return backtest_results

def run_clustering(processed_data: dict) -> dict:
    """ì¢…ëª© í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰"""
    print("\nğŸ¯ ì¢…ëª© í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì¤‘...")
    
    try:
        from src.clustering import (
            prepare_clustering_data, perform_pca, perform_kmeans,
            analyze_clusters, visualize_clusters, generate_cluster_report
        )
        from src.report_generator import save_report
        
        # í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„
        feature_array, symbol_names, metadata = prepare_clustering_data(processed_data)
        
        # PCA ìˆ˜í–‰
        pca_result = perform_pca(feature_array, n_components=2)
        
        # K-Means í´ëŸ¬ìŠ¤í„°ë§
        kmeans_result = perform_kmeans(pca_result['pca_result'], find_optimal=True)
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        cluster_analysis = analyze_clusters(
            symbol_names, kmeans_result['cluster_labels'], 
            feature_array, ['close', 'volume', 'trading_value', 'returns', 'ma_5', 'ma_20', 'volatility'], 
            metadata
        )
        
        # ì‹œê°í™”
        chart_path = visualize_clusters(
            pca_result['pca_result'], 
            kmeans_result['cluster_labels'], 
            symbol_names
        )
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = generate_cluster_report(cluster_analysis, pca_result, kmeans_result, symbol_names)
        report_path = save_report(report, "clustering_report.md")
        
        clustering_results = {
            'cluster_analysis': cluster_analysis,
            'pca_result': pca_result,
            'kmeans_result': kmeans_result,
            'chart_path': chart_path,
            'report_path': report_path
        }
        
        print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {cluster_analysis['total_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°")
        return clustering_results
        
    except Exception as e:
        print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
        return {}

def generate_comprehensive_report(processed_data: dict, 
                                baseline_results: dict,
                                dl_results: dict,
                                backtest_results: dict,
                                clustering_results: dict,
                                config: dict) -> str:
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“Š ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    try:
        from src.report_generator import generate_summary_report
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = {
            'total_symbols': len(processed_data),
            'symbols': list(processed_data.keys()),
            'data_info': {}
        }
        
        for symbol, data in processed_data.items():
            data_summary['data_info'][symbol] = {
                'train_samples': len(data['X_train']),
                'valid_samples': len(data['X_valid']),
                'test_samples': len(data['X_test']),
                'feature_shape': data['X_train'].shape[1:],
                'positive_ratio': data['y_train'].mean()
            }
        
        # ëª¨ë¸ ê²°ê³¼ í†µí•©
        all_model_results = {}
        all_model_results.update(baseline_results)
        all_model_results.update(dl_results)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_path = generate_summary_report(
            data_summary,
            all_model_results,
            backtest_results,
            config,
            include_charts=True
        )
        
        print(f"âœ… ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")
        return report_path
        
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='SectorFlow Lite - AI Trading System')
    parser.add_argument('--mode', choices=['full', 'data', 'train', 'predict', 'backtest', 'report', 'cluster'], 
                       default='full', help='ì‹¤í–‰ ëª¨ë“œ')
    parser.add_argument('--config', default='config.yaml', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--run_id', help='ì‹¤í–‰ ID (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ìƒì„±)')
    parser.add_argument('--dry_run', action='store_true', help='ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ì˜ˆìƒ íŒŒì¼ë§Œ í‘œì‹œ')
    
    args = parser.parse_args()
    
    print("ğŸš€ SectorFlow Lite ì‹œì‘!")
    print("=" * 60)
    print(f"ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
    print(f"ì„¤ì • íŒŒì¼: {args.config}")
    print(f"Dry Run: {args.dry_run}")
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(args.config)
    if not config:
        print("âŒ ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì‹¤í–‰ ID ìƒì„±
    run_id = args.run_id or generate_run_id()
    print(f"ì‹¤í–‰ ID: {run_id}")
    
    # ì‹œë“œ ì„¤ì •
    seed = config.get('train', {}).get('seed', 42)
    set_seeds(seed)
    
    # ì‹¤í–‰ ë””ë ‰í† ë¦¬ ì„¤ì •
    if not args.dry_run:
        runs_dir = setup_run_directory(run_id, config)
    else:
        print("ğŸ” Dry Run ëª¨ë“œ: ì‹¤ì œ íŒŒì¼ ìƒì„± ì—†ì´ ì˜ˆìƒ ì¶œë ¥ë§Œ í‘œì‹œ")
        runs_dir = f"runs/{run_id}"
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = {'run_id': run_id}
    
    try:
        # 1. ë°ì´í„° íŒŒì´í”„ë¼ì¸
        if args.mode in ['full', 'data']:
            processed_data = run_data_pipeline(config)
            results['processed_data'] = processed_data
        else:
            print("âš ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê±´ë„ˆëœ€")
            processed_data = {}
        
        # 2. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨
        if args.mode in ['full', 'train'] and processed_data:
            baseline_results = run_baseline_training(processed_data)
            results['baseline_results'] = baseline_results
        else:
            baseline_results = {}
        
        # 3. ë”¥ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
        if args.mode in ['full', 'train'] and processed_data:
            dl_results = run_deep_learning_training(processed_data, config)
            results['dl_results'] = dl_results
        else:
            dl_results = {}
        
        # 4. ë°±í…ŒìŠ¤íŒ…
        if args.mode in ['full', 'backtest'] and processed_data:
            all_model_results = {**baseline_results, **dl_results}
            backtest_results = run_backtesting(processed_data, all_model_results, config)
            results['backtest_results'] = backtest_results
        else:
            backtest_results = {}
        
        # 5. í´ëŸ¬ìŠ¤í„°ë§
        if args.mode in ['full', 'cluster'] and processed_data:
            clustering_results = run_clustering(processed_data)
            results['clustering_results'] = clustering_results
        else:
            clustering_results = {}
        
        # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        if args.mode in ['full', 'report']:
            report_path = generate_comprehensive_report(
                processed_data, baseline_results, dl_results, 
                backtest_results, clustering_results, config
            )
            results['report_path'] = report_path
        
        # 7. ì‹¤í–‰ ì™„ë£Œ
        print("\nğŸ‰ SectorFlow Lite ì‹¤í–‰ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        if 'processed_data' in results:
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª©: {len(results['processed_data'])}ê°œ")
        if 'baseline_results' in results:
            print(f"ğŸ¤– ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸: {len(results['baseline_results'])}ê°œ ì¢…ëª©")
        if 'dl_results' in results:
            print(f"ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸: {len(results['dl_results'])}ê°œ ì¢…ëª©")
        if 'backtest_results' in results:
            print(f"ğŸ’° ë°±í…ŒìŠ¤íŠ¸ ì „ëµ: {len(results['backtest_results'])}ê°œ")
        if 'clustering_results' in results:
            print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°: {results['clustering_results'].get('cluster_analysis', {}).get('total_clusters', 0)}ê°œ")
        if 'report_path' in results:
            print(f"ğŸ“„ ë¦¬í¬íŠ¸: {results['report_path']}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
