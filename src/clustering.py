#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SectorFlow Lite - Clustering Module
ì¢…ëª© í´ëŸ¬ìŠ¤í„°ë§ ë° ë¶„ì„

Functions:
- prepare_clustering_data: í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ì¤€ë¹„
- perform_pca: PCA ì°¨ì› ì¶•ì†Œ
- perform_kmeans: K-Means í´ëŸ¬ìŠ¤í„°ë§
- analyze_clusters: í´ëŸ¬ìŠ¤í„° ë¶„ì„
- visualize_clusters: í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
- generate_cluster_report: í´ëŸ¬ìŠ¤í„° ë¦¬í¬íŠ¸ ìƒì„±
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, List, Tuple, Optional
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def prepare_clustering_data(processed_data: Dict[str, Any],
                           feature_cols: List[str] = None) -> Tuple[np.ndarray, List[str], Dict[str, Any]]:
    """
    í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ì¤€ë¹„
    
    Args:
        processed_data: ì²˜ë¦¬ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        feature_cols: ì‚¬ìš©í•  í”¼ì²˜ ì»¬ëŸ¼ë“¤
        
    Returns:
        (í”¼ì²˜ ë°°ì—´, ì¢…ëª© ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)
    """
    print("ğŸ”§ í´ëŸ¬ìŠ¤í„°ë§ìš© ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    if feature_cols is None:
        feature_cols = ['close', 'volume', 'trading_value', 'returns', 'ma_5', 'ma_20', 'volatility']
    
    # ê° ì¢…ëª©ì˜ í‰ê·  í”¼ì²˜ê°’ ê³„ì‚°
    symbol_features = []
    symbol_names = []
    metadata = {}
    
    for symbol, data in processed_data.items():
        if 'original_df' in data:
            df = data['original_df']
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
            available_cols = [col for col in feature_cols if col in df.columns]
            
            if available_cols:
                # ê° í”¼ì²˜ì˜ í‰ê· ê°’ ê³„ì‚°
                feature_values = []
                for col in available_cols:
                    if col in df.columns:
                        # NaN ê°’ ì œì™¸í•˜ê³  í‰ê·  ê³„ì‚°
                        mean_val = df[col].dropna().mean()
                        feature_values.append(mean_val if not np.isnan(mean_val) else 0)
                    else:
                        feature_values.append(0)
                
                symbol_features.append(feature_values)
                symbol_names.append(symbol)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata[symbol] = {
                    'feature_values': feature_values,
                    'available_features': available_cols,
                    'data_length': len(df),
                    'last_date': df['date'].max() if 'date' in df.columns else None
                }
    
    # ë°°ì—´ë¡œ ë³€í™˜
    feature_array = np.array(symbol_features)
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   - ì¢…ëª© ìˆ˜: {len(symbol_names)}")
    print(f"   - í”¼ì²˜ ìˆ˜: {feature_array.shape[1]}")
    print(f"   - ì‚¬ìš©ëœ í”¼ì²˜: {available_cols}")
    
    return feature_array, symbol_names, metadata

def perform_pca(feature_array: np.ndarray,
                n_components: int = 2,
                explained_variance_threshold: float = 0.95) -> Dict[str, Any]:
    """
    PCA ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
    
    Args:
        feature_array: í”¼ì²˜ ë°°ì—´
        n_components: ì£¼ì„±ë¶„ ìˆ˜
        explained_variance_threshold: ì„¤ëª… ë¶„ì‚° ì„ê³„ê°’
        
    Returns:
        PCA ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ” PCA ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰ ì¤‘... (ëª©í‘œ: {n_components}ê°œ ì£¼ì„±ë¶„)")
    
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    feature_scaled = scaler.fit_transform(feature_array)
    
    # PCA ìˆ˜í–‰
    pca = PCA(n_components=min(n_components, feature_array.shape[1]))
    pca_result = pca.fit_transform(feature_scaled)
    
    # ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ê³„ì‚°
    explained_variance_ratio = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance_ratio)
    
    # ì¶©ë¶„í•œ ì„¤ëª… ë¶„ì‚°ì„ ê°€ì§„ ì£¼ì„±ë¶„ ìˆ˜ ì°¾ê¸°
    sufficient_components = np.argmax(cumulative_variance >= explained_variance_threshold) + 1
    sufficient_components = min(sufficient_components, feature_array.shape[1])
    
    print(f"âœ… PCA ì™„ë£Œ!")
    print(f"   - ì›ë³¸ ì°¨ì›: {feature_array.shape[1]}")
    print(f"   - ì¶•ì†Œëœ ì°¨ì›: {pca_result.shape[1]}")
    print(f"   - ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {explained_variance_ratio}")
    print(f"   - ëˆ„ì  ì„¤ëª… ë¶„ì‚°: {cumulative_variance}")
    print(f"   - {explained_variance_threshold*100}% ì„¤ëª…ì„ ìœ„í•œ ì£¼ì„±ë¶„ ìˆ˜: {sufficient_components}")
    
    return {
        'pca_result': pca_result,
        'pca_model': pca,
        'scaler': scaler,
        'explained_variance_ratio': explained_variance_ratio,
        'cumulative_variance': cumulative_variance,
        'sufficient_components': sufficient_components
    }

def perform_kmeans(pca_result: np.ndarray,
                   n_clusters: int = 3,
                   max_clusters: int = 10,
                   find_optimal: bool = True) -> Dict[str, Any]:
    """
    K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    
    Args:
        pca_result: PCA ê²°ê³¼
        n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
        max_clusters: ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°ìš©)
        find_optimal: ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ìë™ íƒìƒ‰ ì—¬ë¶€
        
    Returns:
        K-Means ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"ğŸ¯ K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘...")
    
    if find_optimal:
        print("   - ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ ì¤‘...")
        
        # ë‹¤ì–‘í•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ì‹¤í—˜
        cluster_range = range(2, min(max_clusters + 1, len(pca_result)))
        inertias = []
        silhouette_scores = []
        calinski_scores = []
        
        for k in cluster_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(pca_result)
            
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(pca_result, cluster_labels))
            calinski_scores.append(calinski_harabasz_score(pca_result, cluster_labels))
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ (ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ì¤€)
        optimal_k = cluster_range[np.argmax(silhouette_scores)]
        
        print(f"   - ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k} (ì‹¤ë£¨ì—£ ì ìˆ˜: {max(silhouette_scores):.3f})")
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨
        final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        final_labels = final_kmeans.fit_predict(pca_result)
        
        n_clusters = optimal_k
    else:
        # ì§€ì •ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ í›ˆë ¨
        final_kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        final_labels = final_kmeans.fit_predict(pca_result)
        
        inertias = [final_kmeans.inertia_]
        silhouette_scores = [silhouette_score(pca_result, final_labels)]
        calinski_scores = [calinski_harabasz_score(pca_result, final_labels)]
    
    print(f"âœ… K-Means í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
    print(f"   - í´ëŸ¬ìŠ¤í„° ìˆ˜: {n_clusters}")
    print(f"   - ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_scores[-1]:.3f}")
    print(f"   - Calinski-Harabasz ì ìˆ˜: {calinski_scores[-1]:.3f}")
    
    return {
        'cluster_labels': final_labels,
        'kmeans_model': final_kmeans,
        'n_clusters': n_clusters,
        'inertias': inertias,
        'silhouette_scores': silhouette_scores,
        'calinski_scores': calinski_scores,
        'cluster_centers': final_kmeans.cluster_centers_
    }

def analyze_clusters(symbol_names: List[str],
                    cluster_labels: np.ndarray,
                    feature_array: np.ndarray,
                    feature_cols: List[str],
                    metadata: Dict[str, Any]) -> Dict[str, Any]:
    """
    í´ëŸ¬ìŠ¤í„° ë¶„ì„
    
    Args:
        symbol_names: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        cluster_labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨
        feature_array: í”¼ì²˜ ë°°ì—´
        feature_cols: í”¼ì²˜ ì»¬ëŸ¼ëª…
        metadata: ë©”íƒ€ë°ì´í„°
        
    Returns:
        í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼
    """
    print("ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ì¢…ëª© ê·¸ë£¹í™”
    cluster_analysis = {}
    
    for cluster_id in np.unique(cluster_labels):
        cluster_mask = cluster_labels == cluster_id
        cluster_symbols = [symbol_names[i] for i in np.where(cluster_mask)[0]]
        cluster_features = feature_array[cluster_mask]
        
        # í´ëŸ¬ìŠ¤í„° í†µê³„
        cluster_stats = {
            'symbols': cluster_symbols,
            'count': len(cluster_symbols),
            'avg_features': np.mean(cluster_features, axis=0),
            'std_features': np.std(cluster_features, axis=0)
        }
        
        # ê° í”¼ì²˜ë³„ í‰ê· ê°’
        feature_means = {}
        for i, col in enumerate(feature_cols):
            if i < len(cluster_stats['avg_features']):
                feature_means[col] = cluster_stats['avg_features'][i]
        
        cluster_stats['feature_means'] = feature_means
        
        # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
        cluster_characteristics = analyze_cluster_characteristics(
            cluster_symbols, feature_means, metadata
        )
        cluster_stats['characteristics'] = cluster_characteristics
        
        cluster_analysis[f'cluster_{cluster_id}'] = cluster_stats
    
    # ì „ì²´ í´ëŸ¬ìŠ¤í„° ìš”ì•½
    total_analysis = {
        'cluster_analysis': cluster_analysis,
        'total_clusters': len(np.unique(cluster_labels)),
        'total_symbols': len(symbol_names),
        'cluster_distribution': {
            f'cluster_{i}': np.sum(cluster_labels == i) 
            for i in np.unique(cluster_labels)
        }
    }
    
    print("âœ… í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì™„ë£Œ!")
    for cluster_id, stats in cluster_analysis.items():
        print(f"   - {cluster_id}: {stats['count']}ê°œ ì¢…ëª©")
    
    return total_analysis

def analyze_cluster_characteristics(symbols: List[str],
                                  feature_means: Dict[str, float],
                                  metadata: Dict[str, Any]) -> Dict[str, str]:
    """
    í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
    
    Args:
        symbols: í´ëŸ¬ìŠ¤í„° ë‚´ ì¢…ëª©ë“¤
        feature_means: í”¼ì²˜ í‰ê· ê°’
        metadata: ë©”íƒ€ë°ì´í„°
        
    Returns:
        í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì„¤ëª…
    """
    characteristics = []
    
    # ê°€ê²© íŠ¹ì„±
    if 'close' in feature_means:
        avg_price = feature_means['close']
        if avg_price > 100000:
            characteristics.append("ê³ ê°€ì£¼")
        elif avg_price > 50000:
            characteristics.append("ì¤‘ê°€ì£¼")
        else:
            characteristics.append("ì €ê°€ì£¼")
    
    # ê±°ë˜ëŸ‰ íŠ¹ì„±
    if 'volume' in feature_means:
        avg_volume = feature_means['volume']
        if avg_volume > 10000000:
            characteristics.append("ê³ ê±°ë˜ëŸ‰")
        elif avg_volume > 1000000:
            characteristics.append("ì¤‘ê±°ë˜ëŸ‰")
        else:
            characteristics.append("ì €ê±°ë˜ëŸ‰")
    
    # ë³€ë™ì„± íŠ¹ì„±
    if 'volatility' in feature_means:
        avg_volatility = feature_means['volatility']
        if avg_volatility > 0.03:
            characteristics.append("ê³ ë³€ë™ì„±")
        elif avg_volatility > 0.02:
            characteristics.append("ì¤‘ë³€ë™ì„±")
        else:
            characteristics.append("ì €ë³€ë™ì„±")
    
    # ìˆ˜ìµë¥  íŠ¹ì„±
    if 'returns' in feature_means:
        avg_returns = feature_means['returns']
        if avg_returns > 0.01:
            characteristics.append("ìƒìŠ¹ì¶”ì„¸")
        elif avg_returns < -0.01:
            characteristics.append("í•˜ë½ì¶”ì„¸")
        else:
            characteristics.append("íš¡ë³´")
    
    return {
        'description': ", ".join(characteristics) if characteristics else "íŠ¹ì„± ë¶ˆëª…",
        'characteristics': characteristics
    }

def visualize_clusters(pca_result: np.ndarray,
                      cluster_labels: np.ndarray,
                      symbol_names: List[str],
                      save_path: str = None) -> str:
    """
    í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
    
    Args:
        pca_result: PCA ê²°ê³¼
        cluster_labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨
        symbol_names: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        save_path: ì €ì¥ ê²½ë¡œ
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    print("ğŸ“ˆ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì¤‘...")
    
    # 2D ì‹œê°í™”
    plt.figure(figsize=(12, 8))
    
    # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ìƒ‰ìƒ ë‹¤ë¥´ê²Œ í‘œì‹œ
    unique_clusters = np.unique(cluster_labels)
    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))
    
    for i, cluster_id in enumerate(unique_clusters):
        cluster_mask = cluster_labels == cluster_id
        plt.scatter(
            pca_result[cluster_mask, 0], 
            pca_result[cluster_mask, 1],
            c=[colors[i]], 
            label=f'Cluster {cluster_id}',
            alpha=0.7,
            s=100
        )
        
        # ì¢…ëª©ëª… í‘œì‹œ
        for j, symbol in enumerate(symbol_names):
            if cluster_mask[j]:
                plt.annotate(
                    symbol, 
                    (pca_result[j, 0], pca_result[j, 1]),
                    xytext=(5, 5), 
                    textcoords='offset points',
                    fontsize=8,
                    alpha=0.8
                )
    
    plt.xlabel('First Principal Component')
    plt.ylabel('Second Principal Component')
    plt.title('Stock Clustering Visualization (PCA + K-Means)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ì €ì¥
    if save_path is None:
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        save_path = f"reports/charts/cluster_visualization_{timestamp}.png"
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    return save_path

def generate_cluster_report(cluster_analysis: Dict[str, Any],
                          pca_result: Dict[str, Any],
                          kmeans_result: Dict[str, Any],
                          symbol_names: List[str]) -> str:
    """
    í´ëŸ¬ìŠ¤í„° ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        cluster_analysis: í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼
        pca_result: PCA ê²°ê³¼
        kmeans_result: K-Means ê²°ê³¼
        symbol_names: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë¦¬í¬íŠ¸ ë§ˆí¬ë‹¤ìš´ ë¬¸ìì—´
    """
    print("ğŸ“„ í´ëŸ¬ìŠ¤í„° ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    report = f"""# SectorFlow Lite - Stock Clustering Report
**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š Executive Summary

### í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
- **ì´ ì¢…ëª© ìˆ˜:** {cluster_analysis['total_symbols']}ê°œ
- **í´ëŸ¬ìŠ¤í„° ìˆ˜:** {cluster_analysis['total_clusters']}ê°œ
- **PCA ì„¤ëª… ë¶„ì‚°:** {pca_result['cumulative_variance'][-1]:.1%}
- **ì‹¤ë£¨ì—£ ì ìˆ˜:** {kmeans_result['silhouette_scores'][-1]:.3f}

### í´ëŸ¬ìŠ¤í„° ë¶„í¬
"""
    
    # í´ëŸ¬ìŠ¤í„° ë¶„í¬ í…Œì´ë¸”
    for cluster_id, count in cluster_analysis['cluster_distribution'].items():
        percentage = count / cluster_analysis['total_symbols'] * 100
        report += f"- **{cluster_id}:** {count}ê°œ ì¢…ëª© ({percentage:.1f}%)\n"
    
    # ê° í´ëŸ¬ìŠ¤í„° ìƒì„¸ ë¶„ì„
    report += f"""

---

## ğŸ” Detailed Cluster Analysis

"""
    
    for cluster_id, analysis in cluster_analysis['cluster_analysis'].items():
        report += f"""
### {cluster_id.replace('_', ' ').title()}

**ì¢…ëª© ìˆ˜:** {analysis['count']}ê°œ  
**íŠ¹ì„±:** {analysis['characteristics']['description']}

**í¬í•¨ ì¢…ëª©:**
{', '.join(analysis['symbols'])}

**ì£¼ìš” í”¼ì²˜ í‰ê· ê°’:**
"""
        
        for feature, value in analysis['feature_means'].items():
            if isinstance(value, (int, float)):
                report += f"- **{feature}:** {value:,.2f}\n"
        
        report += "\n"
    
    # PCA ë¶„ì„
    report += f"""
---

## ğŸ“ˆ PCA Analysis

### ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨
"""
    
    for i, ratio in enumerate(pca_result['explained_variance_ratio']):
        report += f"- **PC{i+1}:** {ratio:.1%}\n"
    
    report += f"""
### ëˆ„ì  ì„¤ëª… ë¶„ì‚°
"""
    
    for i, cum_var in enumerate(pca_result['cumulative_variance']):
        report += f"- **PC{i+1}ê¹Œì§€:** {cum_var:.1%}\n"
    
    # í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ ì§€í‘œ
    report += f"""
---

## ğŸ¯ Clustering Quality Metrics

- **ì‹¤ë£¨ì—£ ì ìˆ˜:** {kmeans_result['silhouette_scores'][-1]:.3f}
- **Calinski-Harabasz ì ìˆ˜:** {kmeans_result['calinski_scores'][-1]:.3f}
- **í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ìˆ˜:** {kmeans_result['n_clusters']}ê°œ

### ì‹¤ë£¨ì—£ ì ìˆ˜ í•´ì„
"""
    
    silhouette_score = kmeans_result['silhouette_scores'][-1]
    if silhouette_score > 0.7:
        report += "âœ… **ìš°ìˆ˜í•œ í´ëŸ¬ìŠ¤í„°ë§** (0.7 ì´ìƒ)"
    elif silhouette_score > 0.5:
        report += "âœ… **ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°ë§** (0.5-0.7)"
    elif silhouette_score > 0.3:
        report += "âš ï¸ **ë³´í†µ í´ëŸ¬ìŠ¤í„°ë§** (0.3-0.5)"
    else:
        report += "âŒ **ë¶€ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°ë§** (0.3 ë¯¸ë§Œ)"
    
    report += f"""

---

## ğŸ’¡ Investment Insights

### í´ëŸ¬ìŠ¤í„°ë³„ íˆ¬ì ì „ëµ ì œì•ˆ
"""
    
    for cluster_id, analysis in cluster_analysis['cluster_analysis'].items():
        characteristics = analysis['characteristics']['characteristics']
        
        report += f"""
#### {cluster_id.replace('_', ' ').title()}
- **íŠ¹ì„±:** {', '.join(characteristics)}
- **íˆ¬ì ì „ëµ:** """
        
        if 'ê³ ë³€ë™ì„±' in characteristics:
            report += "ë‹¨ê¸° íŠ¸ë ˆì´ë”©, ë†’ì€ ë¦¬ìŠ¤í¬-ìˆ˜ìµ"
        elif 'ì €ë³€ë™ì„±' in characteristics:
            report += "ì•ˆì •ì  ì¥ê¸° íˆ¬ì"
        elif 'ê³ ê±°ë˜ëŸ‰' in characteristics:
            report += "ìœ ë™ì„± ë†’ì€ ì¢…ëª©, ì ê·¹ì  ê±°ë˜"
        else:
            report += "ê· í˜• ì¡íŒ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±"
        
        report += f"\n- **ê¶Œì¥ ë¹„ì¤‘:** {analysis['count']/cluster_analysis['total_symbols']*100:.1f}%\n"
    
    report += f"""

---

## ğŸ“ Notes
- ì´ í´ëŸ¬ìŠ¤í„°ë§ì€ ê³¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¶„ì„ì…ë‹ˆë‹¤.
- ì‹œì¥ ìƒí™© ë³€í™”ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„° íŠ¹ì„±ì´ ë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- íˆ¬ì ê²°ì • ì‹œ ì¶”ê°€ì ì¸ í€ë”ë©˜í„¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.

**Report Generated by:** SectorFlow Lite v1.0
"""
    
    print("âœ… í´ëŸ¬ìŠ¤í„° ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    return report

def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ SectorFlow Lite - Clustering Module í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # ê°€ìƒì˜ ì¢…ëª© ë°ì´í„° ìƒì„±
    symbols = ['005930', '000660', '035420', '005380', '006400', '000270', '035720', '207940']
    n_symbols = len(symbols)
    n_features = 7
    
    # í”¼ì²˜ ë°ì´í„° ìƒì„± (ê° ì¢…ëª©ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì„±)
    feature_data = []
    for i, symbol in enumerate(symbols):
        # ì¢…ëª©ë³„ë¡œ ë‹¤ë¥¸ íŠ¹ì„± ë¶€ì—¬
        base_features = np.random.normal(0, 1, n_features)
        
        # íŠ¹ì • ì¢…ëª©ë“¤ì— íŠ¹ì„± ë¶€ì—¬
        if i < 3:  # ì²« 3ê°œ ì¢…ëª©ì€ ê³ ê°€ì£¼
            base_features[0] += 2  # close (ê°€ê²©)
        elif i < 5:  # ë‹¤ìŒ 2ê°œ ì¢…ëª©ì€ ê³ ê±°ë˜ëŸ‰
            base_features[1] += 2  # volume
        else:  # ë‚˜ë¨¸ì§€ëŠ” ê³ ë³€ë™ì„±
            base_features[6] += 2  # volatility
        
        feature_data.append(base_features)
    
    feature_array = np.array(feature_data)
    feature_cols = ['close', 'volume', 'trading_value', 'returns', 'ma_5', 'ma_20', 'volatility']
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {}
    for i, symbol in enumerate(symbols):
        metadata[symbol] = {
            'feature_values': feature_data[i],
            'available_features': feature_cols,
            'data_length': 100,
            'last_date': '2024-12-31'
        }
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {n_symbols}ê°œ ì¢…ëª©, {n_features}ê°œ í”¼ì²˜")
    
    # 1. PCA ìˆ˜í–‰
    pca_result = perform_pca(feature_array, n_components=2)
    
    # 2. K-Means í´ëŸ¬ìŠ¤í„°ë§
    kmeans_result = perform_kmeans(pca_result['pca_result'], find_optimal=True)
    
    # 3. í´ëŸ¬ìŠ¤í„° ë¶„ì„
    cluster_analysis = analyze_clusters(
        symbols, 
        kmeans_result['cluster_labels'], 
        feature_array, 
        feature_cols, 
        metadata
    )
    
    # 4. ì‹œê°í™”
    chart_path = visualize_clusters(
        pca_result['pca_result'], 
        kmeans_result['cluster_labels'], 
        symbols
    )
    
    # 5. ë¦¬í¬íŠ¸ ìƒì„±
    report = generate_cluster_report(cluster_analysis, pca_result, kmeans_result, symbols)
    
    # 6. ë¦¬í¬íŠ¸ ì €ì¥
    from report_generator import save_report
    report_path = save_report(report, "clustering_report.md")
    
    print(f"\nğŸ“„ ìƒì„±ëœ ë¦¬í¬íŠ¸: {report_path}")
    print(f"ğŸ“ˆ ìƒì„±ëœ ì°¨íŠ¸: {chart_path}")
    print("\nâœ… Clustering Module í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    return {
        'cluster_analysis': cluster_analysis,
        'pca_result': pca_result,
        'kmeans_result': kmeans_result,
        'report_path': report_path,
        'chart_path': chart_path
    }

if __name__ == "__main__":
    results = main()





